{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a .safetensors file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "os.makedirs(\"./contrast_model\", exist_ok=True)\n",
    "\n",
    "# Create a small tensor to simulate model weights\n",
    "tensor = torch.randn(2, 3)  # A simple 2x3 random tensor\n",
    "weights = {\"model_weights\": tensor}\n",
    "\n",
    "# Save it as a safetensor file\n",
    "save_file(weights, \"./contrast_model/model.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/6719a3d2-7cd0-8011-a93c-7ab96a8fc08c\n",
    "\n",
    "curr model structure:\n",
    "\n",
    "```\n",
    ".\n",
    "└── contrast_model\n",
    "    ├── __init__.py\n",
    "    ├── configuration_contrast.py\n",
    "    ├── modeling_contrast.py\n",
    "    ├── model.safetensors\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contrast': 0.9960784316062927}\n"
     ]
    }
   ],
   "source": [
    "from contrast_model.configuration_contrast import ContrastConfig\n",
    "from contrast_model.modeling_contrast import ContrastModel\n",
    "\n",
    "# Create a dummy config and model\n",
    "config = ContrastConfig(image_size=768)\n",
    "model = ContrastModel(config)\n",
    "\n",
    "# Run the model on a sample image (provide your image path here)\n",
    "image_path = \"sample_image.jpg\"\n",
    "output = model.forward(image_path)\n",
    "\n",
    "print(output)  # Prints the contrast value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to hf format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contrast': 0.9960784316062927}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save the config and model (this will also save the model code)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m config\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrast_model_hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontrast_model_hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/transformers/modeling_utils.py:2602\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2599\u001b[0m \u001b[38;5;66;03m# save the string version of dtype to the config, e.g. convert torch.float32 => \"float32\"\u001b[39;00m\n\u001b[1;32m   2600\u001b[0m \u001b[38;5;66;03m# we currently don't use this setting automatically, but may start to use with v5\u001b[39;00m\n\u001b[1;32m   2601\u001b[0m dtype \u001b[38;5;241m=\u001b[39m get_parameter_dtype(model_to_save)\n\u001b[0;32m-> 2602\u001b[0m model_to_save\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtorch_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# Attach architecture to the config\u001b[39;00m\n\u001b[1;32m   2605\u001b[0m model_to_save\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;241m=\u001b[39m [model_to_save\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from contrast_model.configuration_contrast import ContrastConfig\n",
    "from contrast_model.modeling_contrast import ContrastModel\n",
    "\n",
    "# Create a dummy config and model\n",
    "config = ContrastConfig(image_size=768)\n",
    "model = ContrastModel(config)\n",
    "\n",
    "# Manually set torch_dtype for saving compatibility\n",
    "model.config.torch_dtype = \"torch.float32\"  # You can set this as needed\n",
    "\n",
    "# Run the model on a sample image (provide your image path here)\n",
    "image_path = \"sample_image.jpg\"\n",
    "output = model.forward(image_path)\n",
    "print(output)  # Prints the contrast value\n",
    "\n",
    "# Save the config and model (this will also save the model code)\n",
    "config.save_pretrained(\"contrast_model_hf\")\n",
    "model.save_pretrained(\"contrast_model_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = model.config.torch_dtype\n",
    "str(td).split(\".\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model.config.torch_dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
